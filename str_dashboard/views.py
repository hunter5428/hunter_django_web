# str_dashboard/views.py

import json
import logging
from django.contrib.auth.decorators import login_required
from django.http import JsonResponse, HttpResponseBadRequest
from django.shortcuts import render
from django.views.decorators.http import require_POST
from django.conf import settings
from datetime import datetime, timedelta
import pandas as pd
from typing import Optional, Dict, Any
from pathlib import Path
from .orderbook_analyzer import OrderbookAnalyzer


from .db_utils import (
    OracleConnection, 
    OracleConnectionError,
    require_db_connection, 
    execute_query_with_error_handling
)
from .queries.rule_objectives import build_rule_to_objectives
from .queries.rule_historic_search import (
    fetch_df_result_0, 
    aggregate_by_rule_id_list,
    find_most_similar_rule_combinations
)

from .redshift_utils import (
    RedshiftConnection,
    RedshiftConnectionError,
    require_redshift_connection,
    execute_redshift_query_with_error_handling
)


logger = logging.getLogger(__name__)


@login_required
def home(request):
    """í™ˆ í˜ì´ì§€"""
    context = {
        'active_top_menu': '',
        'active_sub_menu': ''
    }
    return render(request, 'str_dashboard/home.html', context)


@login_required
def menu1_1(request):
    """ALERT ID ì¡°íšŒ í˜ì´ì§€"""
    # ì´ì „ ì—°ê²° ì •ë³´ì—ì„œ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ
    default_service = 'PRDAMLKR.OCIAMLPRODDBA.OCIAMLPROD.ORACLEVCN.COM'
    
    db_info = request.session.get('db_conn')
    if db_info and isinstance(db_info, dict):
        jdbc_url = db_info.get('jdbc_url', '')
        if jdbc_url.startswith('jdbc:oracle:thin:@//'):
            try:
                # jdbc:oracle:thin:@//host:port/service_name í˜•ì‹ì—ì„œ service_name ì¶”ì¶œ
                default_service = jdbc_url.split('/', 3)[-1]
            except Exception:
                pass
    # Redshift ì„¸ì…˜ ì •ë³´ ì¶”ê°€
    rs_info = request.session.get('rs_conn')
    # Rule ê°ê´€ì‹ ë§¤í•‘ ë°ì´í„° ìƒì„±
    rule_obj_map = build_rule_to_objectives()
    
    context = {
        'active_top_menu': 'menu1',
        'active_sub_menu': 'menu1_1',
        # Oracle ìƒíƒœ ë° ê¸°ë³¸ê°’
        'db_status': request.session.get('db_conn_status', 'need'),
        'default_host': '127.0.0.1',
        'default_port': '40112',
        'default_service': default_service,
        'default_username': db_info.get('username', '') if db_info else '',
        # Redshift ìƒíƒœ ë° ê¸°ë³¸ê°’ ì¶”ê°€
        'rs_status': request.session.get('rs_conn_status', 'need'),
        'default_rs_host': '127.0.0.1',
        'default_rs_port': '40127',
        'default_rs_dbname': 'prod',
        'default_rs_username': rs_info.get('username', '') if rs_info else '',
        # ê¸°íƒ€
        'rule_obj_map_json': json.dumps(rule_obj_map, ensure_ascii=False),
    }
    return render(request, 'str_dashboard/menu1_1/main.html', context)


@login_required
@require_POST
def test_oracle_connection(request):
    """Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    required_params = ['host', 'port', 'service_name', 'username', 'password']
    params = {}
    
    for param in required_params:
        value = request.POST.get(param, '').strip() if param != 'password' else request.POST.get(param, '')
        if not value:
            return HttpResponseBadRequest(f'Missing parameter: {param}')
        params[param] = value
    
    # JDBC URL ìƒì„±
    jdbc_url = OracleConnection.build_jdbc_url(
        params['host'], 
        params['port'], 
        params['service_name']
    )
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        oracle_conn = OracleConnection(
            jdbc_url=jdbc_url,
            username=params['username'],
            password=params['password']
        )
        
        if oracle_conn.test_connection():
            # ì—°ê²° ì„±ê³µ - ì„¸ì…˜ì— ì €ì¥
            request.session['db_conn_status'] = 'ok'
            request.session['db_conn'] = {
                'jdbc_url': jdbc_url,
                'driver_path': oracle_conn.driver_path,
                'driver_class': oracle_conn.driver_class,
                'username': params['username'],
                'password': params['password'],
            }
            
            # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì„¤ì • (1ì‹œê°„)
            request.session.set_expiry(3600)
            
            logger.info(f"Oracle connection successful for user: {params['username']}")
            return JsonResponse({
                'success': True,
                'message': 'ì—°ê²°ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.'
            })
        else:
            raise OracleConnectionError("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            
    except OracleConnectionError as e:
        request.session['db_conn_status'] = 'need'
        logger.warning(f"Oracle connection failed: {e}")
        return JsonResponse({
            'success': False,
            'message': str(e)
        })
    except Exception as e:
        request.session['db_conn_status'] = 'need'
        logger.exception(f"Unexpected error during connection test: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ì—°ê²° ì‹¤íŒ¨: {str(e)}'
        })


@login_required
@require_POST
@require_db_connection
def query_alert_info(request, oracle_conn=None):
    """ALERT ID ê¸°ë°˜ ì •ë³´ ì¡°íšŒ"""
    alert_id = request.POST.get('alert_id', '').strip()
    if not alert_id:
        return HttpResponseBadRequest('Missing alert_id.')
    
    logger.info(f"Querying alert info for alert_id: {alert_id}")
    
    try:
        # alert_info_by_alert_id.sqlì„ ë³´ë©´ :alert_idê°€ í•œ ë²ˆë§Œ ì‚¬ìš©ë¨
        result = execute_query_with_error_handling(
            oracle_conn=oracle_conn,
            sql_filename='alert_info_by_alert_id.sql',
            bind_params={':alert_id': '?'},
            query_params=[alert_id]
        )
        
        logger.info(f"Alert query result - success: {result.get('success')}, rows: {len(result.get('rows', []))}")
        
        return JsonResponse(result)
        
    except Exception as e:
        logger.exception(f"Error in query_alert_info: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}'
        })


@login_required
@require_POST
@require_db_connection
def query_customer_unified_info(request, oracle_conn=None):
    """í†µí•© ê³ ê° ì •ë³´ ì¡°íšŒ (ê¸°ë³¸ + ìƒì„¸)"""
    cust_id = request.POST.get('cust_id', '').strip()
    
    if not cust_id:
        return HttpResponseBadRequest('Missing cust_id.')
    
    logger.info(f"Querying unified customer info for cust_id: {cust_id}")
    
    # í†µí•© ì¿¼ë¦¬ ì‹¤í–‰
    result = execute_query_with_error_handling(
        oracle_conn=oracle_conn,
        sql_filename='customer_unified_info.sql',
        bind_params={':custId': '?'},
        query_params=[cust_id]
    )
    
    if result.get('success'):
        # ê²°ê³¼ì—ì„œ ê³ ê° êµ¬ë¶„ ì¶”ì¶œ
        columns = result.get('columns', [])
        rows = result.get('rows', [])
        
        customer_type = None
        if rows and len(rows) > 0:
            cust_type_idx = columns.index('ê³ ê°êµ¬ë¶„') if 'ê³ ê°êµ¬ë¶„' in columns else -1
            if cust_type_idx >= 0:
                customer_type = rows[0][cust_type_idx]
        
        # ì‘ë‹µì— ê³ ê° ìœ í˜• ì¶”ê°€
        result['customer_type'] = customer_type
        
        logger.info(f"Unified query successful - customer_type: {customer_type}, rows: {len(rows)}")
    else:
        logger.error(f"Unified query failed: {result.get('message')}")
    
    return JsonResponse(result)


@login_required
@require_POST
@require_db_connection
def rule_history_search(request, oracle_conn=None):
    """
    RULE íˆìŠ¤í† ë¦¬ ê²€ìƒ‰
    
    POST Parameters:
        rule_key: 'ID1,ID2,...' (ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ëœ RULE ID ëª©ë¡)
    """
    rule_key = request.POST.get('rule_key', '').strip()
    if not rule_key:
        return HttpResponseBadRequest('Missing rule_key.')
    
    try:
        # DataFrame ê¸°ë°˜ ì²˜ë¦¬ëŠ” ê¸°ì¡´ í•¨ìˆ˜ í™œìš©
        # ì—°ê²° ì •ë³´ ì¶”ì¶œ
        db_info = request.session.get('db_conn')
        
        # ì „ì²´ ì§‘ê³„ ë°ì´í„° ì¡°íšŒ
        df0 = fetch_df_result_0(
            jdbc_url=db_info['jdbc_url'],
            driver_class=db_info['driver_class'],
            driver_path=db_info['driver_path'],
            username=db_info['username'],
            password=db_info['password']
        )
        
        # ì§‘ê³„ ì²˜ë¦¬
        df1 = aggregate_by_rule_id_list(df0)
        
        # ì¼ì¹˜í•˜ëŠ” í–‰ í•„í„°ë§
        matching_rows = df1[df1["STR_RULE_ID_LIST"] == rule_key]
        
        # ê²°ê³¼ ë³€í™˜
        columns = list(matching_rows.columns) if not matching_rows.empty else list(df1.columns) if not df1.empty else []
        rows = matching_rows.values.tolist()
        
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ìœ ì‚¬ ì¡°í•© ê²€ìƒ‰
        similar_list = []
        if len(rows) == 0 and not df1.empty:
            similar_list = find_most_similar_rule_combinations(rule_key, df1)
        
        logger.info(f"Rule history search completed. Found {len(rows)} matching rows for key: {rule_key}")
        
        if similar_list:
            logger.info(f"Found {len(similar_list)} similar combinations with similarity: {similar_list[0]['similarity']:.2f}")
        
        return JsonResponse({
            'success': True,
            'columns': columns,
            'rows': rows,
            'searched_rule': rule_key,
            'similar_list': similar_list
        })
        
    except Exception as e:
        logger.exception(f"Rule history search failed: {e}")
        return JsonResponse({
            'success': False,
            'message': f'íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}'
        })


@login_required
@require_POST
@require_db_connection
def query_duplicate_unified(request, oracle_conn=None):
    """í†µí•© ì¤‘ë³µ íšŒì› ì¡°íšŒ - ì´ë©”ì¼ ì œì™¸ ë²„ì „"""
    
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    current_cust_id = request.POST.get('current_cust_id', '').strip()
    # full_emailì€ ë°›ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í–¥í›„ ë³µêµ¬ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ìœ ì§€)
    full_email = request.POST.get('full_email', '').strip() or None
    phone_suffix = request.POST.get('phone_suffix', '').strip() or None
    address = request.POST.get('address', '').strip() or None
    detail_address = request.POST.get('detail_address', '').strip() or None
    workplace_name = request.POST.get('workplace_name', '').strip() or None
    workplace_address = request.POST.get('workplace_address', '').strip() or None
    workplace_detail_address = request.POST.get('workplace_detail_address', '').strip() or None
    
    if not current_cust_id:
        return JsonResponse({
            'success': True,
            'columns': [],
            'rows': []
        })
    
    # íŒŒë¼ë¯¸í„° ë¡œê¹… (ì´ë©”ì¼ ì œì™¸)
    logger.debug(f"Duplicate search params - cust_id: {current_cust_id}")
    logger.debug(f"  phone: {phone_suffix}")
    logger.debug(f"  address: {bool(address)}, workplace: {bool(workplace_name)}")
    logger.info("Note: Email-based duplicate search is currently disabled")
    
    # í†µí•© ì¿¼ë¦¬ ì‹¤í–‰ (ì´ë©”ì¼ íŒŒë¼ë¯¸í„° ì œì™¸)
    result = execute_query_with_error_handling(
        oracle_conn=oracle_conn,
        sql_filename='duplicate_unified.sql',
        bind_params={
            ':current_cust_id': '?',
            ':address': '?',
            ':detail_address': '?',
            ':workplace_name': '?',
            ':workplace_address': '?',
            ':workplace_detail_address': '?',
            ':phone_suffix': '?'
        },
        query_params=[
            # ì£¼ì†Œ ì¡°ê±´ (4ê°œ)
            current_cust_id,
            address,
            address,
            detail_address,
            
            # ì§ì¥ëª… ì¡°ê±´ (3ê°œ)
            current_cust_id,
            workplace_name,
            workplace_name,
            
            # ì§ì¥ì£¼ì†Œ ì¡°ê±´ (5ê°œ)
            current_cust_id,
            workplace_address,
            workplace_address,
            workplace_detail_address,
            workplace_detail_address,
            
            # ì „í™”ë²ˆí˜¸ í•„í„° (2ê°œ)
            phone_suffix,
            phone_suffix
        ]  # ì´ 14ê°œ
    )
    
    if result.get('success'):
        logger.info(f"Duplicate search successful - found {len(result.get('rows', []))} records")
        
        # ì„¸ì…˜ì— ì €ì¥ (ì¶”ê°€)
        request.session['duplicate_persons_data'] = {
            'columns': result.get('columns', []),
            'rows': result.get('rows', [])
        }
        request.session.modified = True
        
        if full_email:
            logger.info("Note: Email was provided but not used in search due to encryption issues")
    
    return JsonResponse(result)


@login_required
@require_POST
@require_db_connection
def query_corp_related_persons(request, oracle_conn=None):
    """ë²•ì¸ ê´€ë ¨ì¸ ì •ë³´ ì¡°íšŒ"""
    cust_id = request.POST.get('cust_id', '').strip()
    
    if not cust_id:
        return HttpResponseBadRequest('Missing cust_id.')
    
    logger.info(f"Querying corp related persons for cust_id: {cust_id}")
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    result = execute_query_with_error_handling(
        oracle_conn=oracle_conn,
        sql_filename='corp_related_persons.sql',
        bind_params={':cust_id': '?'},
        query_params=[cust_id]
    )
    
    if result.get('success'):
        logger.info(f"Corp related persons query successful - found {len(result.get('rows', []))} persons")
    else:
        logger.error(f"Corp related persons query failed: {result.get('message')}")
    
    return JsonResponse(result)


@login_required
@require_POST
@require_db_connection
def query_person_related_summary(request, oracle_conn=None):
    """ê°œì¸ ê³ ê°ì˜ ê´€ë ¨ì¸(ë‚´ë¶€ì…ì¶œê¸ˆ ê±°ë˜ ìƒëŒ€ë°©) ì •ë³´ ì¡°íšŒ"""
    
    cust_id = request.POST.get('cust_id', '').strip()
    start_date = request.POST.get('start_date', '').strip()
    end_date = request.POST.get('end_date', '').strip()
    
    if not all([cust_id, start_date, end_date]):
        return JsonResponse({
            'success': False,
            'message': 'Missing required parameters: cust_id, start_date, end_date'
        })
    
    logger.info(f"Querying person related summary - cust_id: {cust_id}, period: {start_date} ~ {end_date}")
    
    try:
        # person_related_summary.sql ì‹¤í–‰
        result = execute_query_with_error_handling(
            oracle_conn=oracle_conn,
            sql_filename='person_related_summary.sql',
            bind_params={
                ':cust_id': '?',
                ':start_date': '?',
                ':end_date': '?'
            },
            query_params=[
                start_date,     # ì²« ë²ˆì§¸ :start_date
                end_date,       # ì²« ë²ˆì§¸ :end_date
                cust_id,        # ì²« ë²ˆì§¸ :cust_id
                cust_id,        # ë‘ ë²ˆì§¸ :cust_id
                start_date,     # ë‘ ë²ˆì§¸ :start_date
                end_date        # ë‘ ë²ˆì§¸ :end_date
            ]
        )
        
        if not result.get('success'):
            logger.error(f"Person related summary query failed: {result.get('message')}")
            return JsonResponse(result)
        
        # ê²°ê³¼ ë°ì´í„° ì²˜ë¦¬ ë° í¬ë§·íŒ…
        columns = result.get('columns', [])
        rows = result.get('rows', [])
        
        # ê´€ë ¨ì¸ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
        related_persons = {}
        
        for row in rows:
            record_type = row[0] if len(row) > 0 else None
            cust_id_val = row[1] if len(row) > 1 else None
            
            if not cust_id_val:
                continue
            
            if cust_id_val not in related_persons:
                related_persons[cust_id_val] = {
                    'info': None,
                    'transactions': []
                }
            
            if record_type == 'PERSON_INFO':
                # ê°œì¸ ì •ë³´ ë ˆì½”ë“œ
                related_persons[cust_id_val]['info'] = {
                    'cust_id': cust_id_val,
                    'name': row[2],
                    'id_number': row[3],
                    'birth_date': row[4],
                    'age': row[5],
                    'gender': row[6],
                    'address': row[7],
                    'job': row[8],
                    'workplace': row[9],
                    'workplace_addr': row[10],
                    'income_source': row[11],
                    'tran_purpose': row[12],
                    'risk_grade': row[13],
                    'total_tran_count': row[14]
                }
            elif record_type == 'TRAN_SUMMARY':
                # ê±°ë˜ ìš”ì•½ ë ˆì½”ë“œ
                related_persons[cust_id_val]['transactions'].append({
                    'coin_symbol': row[15],
                    'tran_type': row[16],
                    'tran_qty': row[17],
                    'tran_amt': row[18],
                    'tran_cnt': row[19]
                })
        
        # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        summary_text = format_related_person_summary(related_persons)
        
        logger.info(f"Person related summary completed - found {len(related_persons)} related persons")
        
        return JsonResponse({
            'success': True,
            'related_persons': related_persons,
            'summary_text': summary_text,
            'raw_columns': columns,
            'raw_rows': rows
        })
        
    except Exception as e:
        logger.exception(f"Error in query_person_related_summary: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ê´€ë ¨ì¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}'
        })


def format_related_person_summary(related_persons):
    """ê´€ë ¨ì¸ ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    if not related_persons:
        return "ë‚´ë¶€ì…ì¶œê¸ˆ ê±°ë˜ ê´€ë ¨ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
    
    lines = []
    lines.append("=" * 80)
    lines.append("ã€ ê°œì¸ ê³ ê° ê´€ë ¨ì¸ ì •ë³´ (ë‚´ë¶€ì…ì¶œê¸ˆ ê±°ë˜ ìƒëŒ€ë°©) ã€‘")
    lines.append("=" * 80)
    lines.append("")
    
    for idx, (cust_id, data) in enumerate(related_persons.items(), 1):
        info = data.get('info')
        transactions = data.get('transactions', [])
        
        if not info:
            continue
        
        # ê´€ë ¨ì¸ ê¸°ë³¸ ì •ë³´
        lines.append(f"â—† ê´€ë ¨ì¸ {idx}: {info.get('name', 'N/A')} (CID: {cust_id})")
        lines.append("-" * 60)
        
        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        lines.append(f"  â€¢ ì‹¤ëª…ë²ˆí˜¸: {info.get('id_number', 'N/A')}")
        lines.append(f"  â€¢ ìƒë…„ì›”ì¼: {info.get('birth_date', 'N/A')} (ë§Œ {info.get('age', 'N/A')}ì„¸)")
        lines.append(f"  â€¢ ì„±ë³„: {info.get('gender', 'N/A')}")
        lines.append(f"  â€¢ ê±°ì£¼ì§€: {info.get('address', 'N/A')}")
        
        if info.get('job'):
            lines.append(f"  â€¢ ì§ì—…: {info.get('job')}")
        if info.get('workplace'):
            lines.append(f"  â€¢ ì§ì¥ëª…: {info.get('workplace')}")
        if info.get('workplace_addr'):
            lines.append(f"  â€¢ ì§ì¥ì£¼ì†Œ: {info.get('workplace_addr')}")
        
        lines.append(f"  â€¢ ìê¸ˆì˜ ì›ì²œ: {info.get('income_source', 'N/A')}")
        lines.append(f"  â€¢ ê±°ë˜ëª©ì : {info.get('tran_purpose', 'N/A')}")
        lines.append(f"  â€¢ ìœ„í—˜ë“±ê¸‰: {info.get('risk_grade', 'N/A')}")
        lines.append(f"  â€¢ ì´ ê±°ë˜íšŸìˆ˜: {info.get('total_tran_count', 0)}íšŒ")
        lines.append("")
        
        # ê±°ë˜ ë‚´ì—­ ìš”ì•½
        if transactions:
            lines.append("  â–¶ ê±°ë˜ ë‚´ì—­ (ì¢…ëª©ë³„)")
            lines.append("  " + "-" * 56)
            
            # ë‚´ë¶€ì…ê³ /ì¶œê³  ë¶„ë¦¬
            deposits = [t for t in transactions if t.get('tran_type') == 'ë‚´ë¶€ì…ê³ ']
            withdrawals = [t for t in transactions if t.get('tran_type') == 'ë‚´ë¶€ì¶œê³ ']
            
            if deposits:
                lines.append("  [ë‚´ë¶€ì…ê³ ]")
                for t in sorted(deposits, key=lambda x: float(x.get('tran_amt', 0) or 0), reverse=True):
                    qty = float(t.get('tran_qty', 0) or 0)
                    amt = float(t.get('tran_amt', 0) or 0)
                    cnt = int(t.get('tran_cnt', 0) or 0)
                    lines.append(f"    - {t.get('coin_symbol', 'N/A')}: "
                               f"ìˆ˜ëŸ‰ {qty:,.4f}, "
                               f"ê¸ˆì•¡ {amt:,.0f}ì›, "
                               f"ê±´ìˆ˜ {cnt}ê±´")
            
            if withdrawals:
                lines.append("  [ë‚´ë¶€ì¶œê³ ]")
                for t in sorted(withdrawals, key=lambda x: float(x.get('tran_amt', 0) or 0), reverse=True):
                    qty = float(t.get('tran_qty', 0) or 0)
                    amt = float(t.get('tran_amt', 0) or 0)
                    cnt = int(t.get('tran_cnt', 0) or 0)
                    lines.append(f"    - {t.get('coin_symbol', 'N/A')}: "
                               f"ìˆ˜ëŸ‰ {qty:,.4f}, "
                               f"ê¸ˆì•¡ {amt:,.0f}ì›, "
                               f"ê±´ìˆ˜ {cnt}ê±´")
        
        lines.append("")
    
    lines.append("=" * 80)
    
    return "\n".join(lines)



# views.py Part 2 - query_ip_access_historyë¶€í„° ëê¹Œì§€

@login_required
@require_POST
@require_db_connection
def query_ip_access_history(request, oracle_conn=None):
    """IP ì ‘ì† ì´ë ¥ ì¡°íšŒ"""
    mem_id = request.POST.get('mem_id', '').strip()
    start_date = request.POST.get('start_date', '').strip()
    end_date = request.POST.get('end_date', '').strip()
    
    if not all([mem_id, start_date, end_date]):
        return JsonResponse({
            'success': False,
            'message': 'Missing required parameters: mem_id, start_date, end_date'
        })
    
    logger.info(f"Querying IP access history - MID: {mem_id}, period: {start_date} ~ {end_date}")
    
    try:
        # query_ip_access_history.sql ì‹¤í–‰
        result = execute_query_with_error_handling(
            oracle_conn=oracle_conn,
            sql_filename='query_ip_access_history.sql',
            bind_params={
                ':mem_id': '?',
                ':start_date': '?', 
                ':end_date': '?'
            },
            query_params=[mem_id, start_date, end_date]
        )
        
        if result.get('success'):
            rows = result.get('rows', [])
            columns = result.get('columns', [])
            
            # ì„¸ì…˜ì— ì €ì¥ (ìˆœì„œ ì¡°ì • - ë¡œê¹… ì „ì— ì €ì¥)
            request.session['ip_history_data'] = {
                'columns': columns,
                'rows': rows
            }
            request.session.modified = True
            
            # í•´ì™¸ ì ‘ì† ê±´ìˆ˜ ë¡œê¹…
            if rows and columns:
                country_idx = columns.index('êµ­ê°€í•œê¸€ëª…') if 'êµ­ê°€í•œê¸€ëª…' in columns else -1
                if country_idx >= 0:
                    foreign_count = sum(1 for row in rows 
                                      if row[country_idx] and 
                                      row[country_idx] not in ['ëŒ€í•œë¯¼êµ­', 'í•œêµ­'])
                    if foreign_count > 0:
                        logger.info(f"Found {foreign_count} foreign IP access records out of {len(rows)} total")
            
            logger.info(f"IP access history query successful - found {len(rows)} records")
        else:
            logger.error(f"IP access history query failed: {result.get('message')}")
        
        return JsonResponse(result)
        
    except Exception as e:
        logger.exception(f"Error in query_ip_access_history: {e}")
        return JsonResponse({
            'success': False,
            'message': f'IP ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}'
        })


@login_required
@require_POST
def test_redshift_connection(request):
    """Redshift ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    required_params = ['host', 'port', 'dbname', 'username', 'password']
    params = {}
    
    for param in required_params:
        value = request.POST.get(param, '').strip() if param != 'password' else request.POST.get(param, '')
        if not value:
            return JsonResponse({
                'success': False,
                'message': f'{param}ì„(ë¥¼) ì…ë ¥í•´ì£¼ì„¸ìš”.'
            })
        params[param] = value
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        redshift_conn = RedshiftConnection(
            host=params['host'],
            port=params['port'],
            dbname=params['dbname'],
            username=params['username'],
            password=params['password']
        )
        
        if redshift_conn.test_connection():
            # ì—°ê²° ì„±ê³µ - ì„¸ì…˜ì— ì €ì¥
            request.session['rs_conn_status'] = 'ok'
            request.session['rs_conn'] = {
                'host': params['host'],
                'port': params['port'],
                'dbname': params['dbname'],
                'username': params['username'],
                'password': params['password'],
            }
            
            logger.info(f"Redshift connection successful for user: {params['username']}")
            return JsonResponse({
                'success': True,
                'message': 'Redshift ì—°ê²°ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.'
            })
        else:
            raise RedshiftConnectionError("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            
    except RedshiftConnectionError as e:
        request.session['rs_conn_status'] = 'need'
        logger.warning(f"Redshift connection failed: {e}")
        return JsonResponse({
            'success': False,
            'message': str(e)
        })
    except Exception as e:
        request.session['rs_conn_status'] = 'need'
        logger.exception(f"Unexpected error during Redshift connection test: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ì—°ê²° ì‹¤íŒ¨: {str(e)}'
        })


@login_required
@require_POST
def connect_all_databases(request):
    """Oracleê³¼ Redshift ëª¨ë‘ ì—°ê²°"""
    oracle_status = 'fail'
    oracle_error = None
    redshift_status = 'fail'
    redshift_error = None
    
    # Oracle ì—°ê²°
    try:
        oracle_params = {
            'host': request.POST.get('oracle_host', '').strip(),
            'port': request.POST.get('oracle_port', '').strip(),
            'service_name': request.POST.get('oracle_service_name', '').strip(),
            'username': request.POST.get('oracle_username', '').strip(),
            'password': request.POST.get('oracle_password', ''),
        }
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        for key, value in oracle_params.items():
            if not value:
                oracle_error = f'Oracle {key} ëˆ„ë½'
                raise ValueError(oracle_error)
        
        # JDBC URL ìƒì„±
        jdbc_url = OracleConnection.build_jdbc_url(
            oracle_params['host'], 
            oracle_params['port'], 
            oracle_params['service_name']
        )
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        oracle_conn = OracleConnection(
            jdbc_url=jdbc_url,
            username=oracle_params['username'],
            password=oracle_params['password']
        )
        
        if oracle_conn.test_connection():
            # ì„¸ì…˜ ì €ì¥
            request.session['db_conn_status'] = 'ok'
            request.session['db_conn'] = {
                'jdbc_url': jdbc_url,
                'driver_path': oracle_conn.driver_path,
                'driver_class': oracle_conn.driver_class,
                'username': oracle_params['username'],
                'password': oracle_params['password'],
            }
            oracle_status = 'ok'
            logger.info(f"Oracle connected: {oracle_params['username']}")
        else:
            oracle_error = 'Oracle ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨'
            
    except Exception as e:
        oracle_error = str(e)
        request.session['db_conn_status'] = 'need'
        logger.error(f"Oracle connection failed: {e}")
    
    # Redshift ì—°ê²°
    try:
        redshift_params = {
            'host': request.POST.get('redshift_host', '').strip(),
            'port': request.POST.get('redshift_port', '').strip(),
            'dbname': request.POST.get('redshift_dbname', '').strip(),
            'username': request.POST.get('redshift_username', '').strip(),
            'password': request.POST.get('redshift_password', ''),
        }
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        for key, value in redshift_params.items():
            if not value:
                redshift_error = f'Redshift {key} ëˆ„ë½'
                raise ValueError(redshift_error)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        redshift_conn = RedshiftConnection(
            host=redshift_params['host'],
            port=redshift_params['port'],
            dbname=redshift_params['dbname'],
            username=redshift_params['username'],
            password=redshift_params['password']
        )
        
        if redshift_conn.test_connection():
            # ì„¸ì…˜ ì €ì¥
            request.session['rs_conn_status'] = 'ok'
            request.session['rs_conn'] = redshift_params
            redshift_status = 'ok'
            logger.info(f"Redshift connected: {redshift_params['username']}")
        else:
            redshift_error = 'Redshift ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨'
            
    except Exception as e:
        redshift_error = str(e)
        request.session['rs_conn_status'] = 'need'
        logger.error(f"Redshift connection failed: {e}")
    
    # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì„¤ì • (1ì‹œê°„)
    request.session.set_expiry(3600)
    
    # ê²°ê³¼ ë°˜í™˜
    if oracle_status == 'ok' and redshift_status == 'ok':
        return JsonResponse({
            'success': True,
            'message': 'ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ',
            'oracle_status': oracle_status,
            'redshift_status': redshift_status
        })
    else:
        return JsonResponse({
            'success': False,
            'message': 'ì¼ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨',
            'oracle_status': oracle_status,
            'oracle_error': oracle_error,
            'redshift_status': redshift_status,
            'redshift_error': redshift_error
        })


def get_db_status(request) -> dict:
    """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ ì¡°íšŒ (Oracle + Redshift)"""
    # Oracle ìƒíƒœ
    db_info = request.session.get('db_conn')
    oracle_status = request.session.get('db_conn_status', 'need')
    
    # Redshift ìƒíƒœ
    rs_info = request.session.get('rs_conn')
    redshift_status = request.session.get('rs_conn_status', 'need')
    
    return {
        'oracle': {
            'connected': oracle_status == 'ok',
            'status': oracle_status,
            'username': db_info.get('username', 'Unknown') if db_info else None,
            'jdbc_url': db_info.get('jdbc_url', '') if db_info else None
        },
        'redshift': {
            'connected': redshift_status == 'ok',
            'status': redshift_status,
            'username': rs_info.get('username', 'Unknown') if rs_info else None,
            'host': rs_info.get('host', '') if rs_info else None,
            'dbname': rs_info.get('dbname', '') if rs_info else None
        }
    }


@login_required
def check_db_status(request):
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸ API (AJAXìš©)"""
    return JsonResponse(get_db_status(request))


def clear_db_session(request):
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™” (Oracle + Redshift)"""
    keys_to_clear = ['db_conn', 'db_conn_status', 'rs_conn', 'rs_conn_status']
    for key in keys_to_clear:
        if key in request.session:
            del request.session[key]
    logger.info("All database sessions cleared")


# ì „ì—­ ë³€ìˆ˜ë¡œ DataFrame ì €ì¥ì†Œ ì¶”ê°€ (í´ë˜ìŠ¤ë‚˜ ìºì‹œ ì‹œìŠ¤í…œìœ¼ë¡œ ê°œì„  ê°€ëŠ¥)
ORDERBOOK_CACHE = {}


@login_required
@require_POST
def query_redshift_orderbook(request):
    """
    Redshiftì—ì„œ Orderbook ë°ì´í„° ì¡°íšŒ
    Alert ë°ì´í„°ì˜ ê±°ë˜ ê¸°ê°„ + 1ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ
    íŠ¹ì • RULE ID (IO000, IO111)ì˜ ê²½ìš° 12ê°œì›” ì´ì „ ë°ì´í„° ì¡°íšŒ
    """
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    user_id = request.POST.get('user_id', '').strip()
    tran_start = request.POST.get('tran_start', '').strip()  # YYYY-MM-DD í˜•ì‹
    tran_end = request.POST.get('tran_end', '').strip()      # YYYY-MM-DD í˜•ì‹
    
    if not all([user_id, tran_start, tran_end]):
        return JsonResponse({
            'success': False,
            'message': 'Missing required parameters: user_id, tran_start, tran_end'
        })
    
    # Redshift ì—°ê²° í™•ì¸
    rs_info = request.session.get('rs_conn')
    if not rs_info or request.session.get('rs_conn_status') != 'ok':
        return JsonResponse({
            'success': False,
            'message': 'Redshift ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.'
        })
    
    try:
        # ë‚ ì§œ íŒŒì‹± ë° +1ì¼ ì²˜ë¦¬
        start_date = datetime.strptime(tran_start, '%Y-%m-%d')
        end_date = datetime.strptime(tran_end, '%Y-%m-%d')
        
        # +1ì¼ ì ìš©
        start_date_plus1 = start_date + timedelta(days=1)
        end_date_plus1 = end_date + timedelta(days=1)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        start_time = start_date_plus1.strftime('%Y-%m-%d 00:00:00')
        end_time = end_date_plus1.strftime('%Y-%m-%d 23:59:59')
        
        logger.info(f"Querying Redshift orderbook - user_id: {user_id}, period: {start_time} ~ {end_time}")
        
        # Redshift ì—°ê²° ìƒì„±
        redshift_conn = RedshiftConnection.from_session(rs_info)
        
        # SQL íŒŒì¼ ë¡œë“œ
        sql_path = Path(settings.BASE_DIR) / 'str_dashboard' / 'queries' / 'redshift_orderbook.sql'
        with open(sql_path, 'r', encoding='utf-8') as f:
            sql_query = f.read()
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        cols, rows = redshift_conn.execute_query(
            sql_query,
            params=[start_time, end_time, user_id]
        )
        
        if not cols or not rows:
            logger.info(f"No orderbook data found for user_id: {user_id}")
            return JsonResponse({
                'success': True,
                'message': 'No data found',
                'rows_count': 0,
                'cached': False
            })
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(rows, columns=cols)
        
        # ìºì‹œ í‚¤ ìƒì„± (ë‚ ì§œ í˜•ì‹ í†µì¼)
        cache_key = f"df_orderbook_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_{user_id}"
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥
        ORDERBOOK_CACHE[cache_key] = {
            'dataframe': df,
            'created_at': datetime.now(),
            'user_id': user_id,
            'start_date': start_date.strftime('%Y-%m-%d'),
            'end_date': end_date.strftime('%Y-%m-%d'),
            'start_time': start_time,
            'end_time': end_time,
            'rows_count': len(df),
            'columns': list(df.columns)
        }
        
        logger.info(f"Orderbook data cached with key: {cache_key}, rows: {len(df)}")
        
        # ì‘ë‹µ ìƒì„± (í˜„ì¬ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜)
        return JsonResponse({
            'success': True,
            'message': f'Orderbook(ê±°ë˜ì›ì¥) ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.',
            'cache_key': cache_key,
            'rows_count': len(df),
            'columns': list(df.columns),
            'period': {
                'original': f"{tran_start} ~ {tran_end}",
                'queried': f"{start_time} ~ {end_time}"
            },
            'cached': True
        })
        
    except Exception as e:
        logger.exception(f"Error in query_redshift_orderbook: {e}")
        return JsonResponse({
            'success': False,
            'message': f'Orderbook ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}'
        })


@login_required
def get_cached_orderbook_info(request):
    """
    ìºì‹œëœ Orderbook ì •ë³´ ì¡°íšŒ
    """
    cache_info = []
    
    for key, value in ORDERBOOK_CACHE.items():
        cache_info.append({
            'cache_key': key,
            'user_id': value['user_id'],
            'period': f"{value['start_date']} ~ {value['end_date']}",
            'rows_count': value['rows_count'],
            'created_at': value['created_at'].strftime('%Y-%m-%d %H:%M:%S'),
            'columns_count': len(value['columns'])
        })
    
    return JsonResponse({
        'success': True,
        'cache_count': len(cache_info),
        'cached_data': cache_info
    })


@login_required
@require_POST
def clear_orderbook_cache(request):
    """
    Orderbook ìºì‹œ ì´ˆê¸°í™”
    """
    cache_key = request.POST.get('cache_key', '').strip()
    
    if cache_key and cache_key in ORDERBOOK_CACHE:
        # íŠ¹ì • ìºì‹œë§Œ ì‚­ì œ
        del ORDERBOOK_CACHE[cache_key]
        logger.info(f"Cleared orderbook cache: {cache_key}")
        message = f'ìºì‹œ {cache_key}ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.'
    elif not cache_key:
        # ì „ì²´ ìºì‹œ ì‚­ì œ
        count = len(ORDERBOOK_CACHE)
        ORDERBOOK_CACHE.clear()
        logger.info(f"Cleared all orderbook cache ({count} items)")
        message = f'ì „ì²´ ìºì‹œ {count}ê°œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.'
    else:
        return JsonResponse({
            'success': False,
            'message': 'í•´ë‹¹ ìºì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
        })
    
    return JsonResponse({
        'success': True,
        'message': message
    })


def get_orderbook_dataframe(cache_key: str) -> Optional[pd.DataFrame]:
    """
    ìºì‹œì—ì„œ DataFrame ê°€ì ¸ì˜¤ê¸° (ë‚´ë¶€ ì‚¬ìš©ìš©)
    """
    if cache_key in ORDERBOOK_CACHE:
        return ORDERBOOK_CACHE[cache_key]['dataframe']
    return None


@login_required
@require_POST
def analyze_cached_orderbook(request):
    """
    ìºì‹œëœ Orderbook ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ ìš”ì•½ ìƒì„± (êµ¬ê°„ ë¶„ì„ ì œì™¸)
    """
    cache_key = request.POST.get('cache_key', '').strip()
    
    if not cache_key:
        return JsonResponse({
            'success': False,
            'message': 'cache_key is required'
        })
    
    # ìºì‹œì—ì„œ DataFrame ê°€ì ¸ì˜¤ê¸°
    df = get_orderbook_dataframe(cache_key)
    
    if df is None:
        return JsonResponse({
            'success': False,
            'message': f'ìºì‹œëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cache_key}'
        })
    
    try:
        # ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
        analyzer = OrderbookAnalyzer(df)
        
        # ë¶„ì„ ì‹¤í–‰ (êµ¬ê°„ ë¶„ì„ ì œì™¸)
        analyzer.analyze()
        
        # í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
        text_summary = analyzer.generate_text_summary()
        
        # íŒ¨í„´ ë¶„ì„
        patterns = analyzer.get_pattern_analysis()
        
        # ì¼ìë³„ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        daily_summary = analyzer.get_daily_summary()
        
        # ğŸ”¥ ìˆ˜ì •: ìºì‹œì— ì €ì¥ëœ ì¡°íšŒ ê¸°ê°„ ì •ë³´ ì‚¬ìš© (ì‹¤ì œ ë°ì´í„° ê¸°ê°„ì´ ì•„ë‹Œ)
        period_info = {}
        if cache_key in ORDERBOOK_CACHE:
            cache_data = ORDERBOOK_CACHE[cache_key]
            # ìºì‹œì— ì €ì¥ëœ ì›ë³¸ ì¡°íšŒ ê¸°ê°„ ì‚¬ìš© (D+1 ì ìš© ì „ì˜ ë‚ ì§œ)
            period_info = {
                'start_date': cache_data['start_date'],  # ì´ë¯¸ -3ê°œì›” ë˜ëŠ” -12ê°œì›” ì ìš©ëœ ë‚ ì§œ
                'end_date': cache_data['end_date'],      # ALERTì˜ TRAN_END ë‚ ì§œ
                'query_start': cache_data['start_time'], # ì‹¤ì œ ì¿¼ë¦¬ì— ì‚¬ìš©ëœ ì‹œê°„ (D+1 ì ìš©)
                'query_end': cache_data['end_time']      # ì‹¤ì œ ì¿¼ë¦¬ì— ì‚¬ìš©ëœ ì‹œê°„ (D+1 ì ìš©)
            }
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì¶”ê°€ ì €ì¥
        if cache_key in ORDERBOOK_CACHE:
            ORDERBOOK_CACHE[cache_key]['analysis'] = {
                'text_summary': text_summary,
                'patterns': patterns,
                'daily_summary': daily_summary,
                'period_info': period_info,
                'analyzed_at': datetime.now()
            }
        
        logger.info(f"Orderbook analysis completed for {cache_key}")
        
        # DataFrameì„ JSONìœ¼ë¡œ ë³€í™˜
        daily_json = daily_summary.to_dict('records') if not daily_summary.empty else []
        
        return JsonResponse({
            'success': True,
            'cache_key': cache_key,
            'daily_summary': daily_json,
            'text_summary': text_summary,
            'patterns': patterns,
            'period_info': period_info
        })
        
    except Exception as e:
        logger.exception(f"Error analyzing orderbook: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        })


@login_required
def get_orderbook_summary(request):
    """
    ìºì‹œëœ Orderbookì˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (êµ¬ê°„ ë¶„ì„ ì œì™¸)
    """
    cache_key = request.GET.get('cache_key', '').strip()
    
    if not cache_key:
        return JsonResponse({
            'success': False,
            'message': 'cache_key is required'
        })
    
    if cache_key not in ORDERBOOK_CACHE:
        return JsonResponse({
            'success': False,
            'message': f'ìºì‹œëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cache_key}'
        })
    
    cache_data = ORDERBOOK_CACHE[cache_key]
    
    # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'analysis' not in cache_data:
        return JsonResponse({
            'success': False,
            'message': 'ë¶„ì„ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.',
            'analyzed': False
        })
    
    analysis = cache_data['analysis']
    
    return JsonResponse({
        'success': True,
        'cache_key': cache_key,
        'user_id': cache_data['user_id'],
        'period': f"{cache_data['start_date']} ~ {cache_data['end_date']}",
        'rows_count': cache_data['rows_count'],
        'text_summary': analysis['text_summary'],
        'patterns': analysis['patterns'],
        'analyzed_at': analysis['analyzed_at'].strftime('%Y-%m-%d %H:%M:%S'),
        'analyzed': True
    })


@login_required
@require_POST
def analyze_alert_orderbook(request):
    """
    íŠ¹ì • ALERT IDì— ëŒ€í•œ Orderbook ìƒì„¸ ë¶„ì„
    """
    alert_id = request.POST.get('alert_id', '').strip()
    start_date = request.POST.get('start_date', '').strip()
    end_date = request.POST.get('end_date', '').strip()
    cache_key = request.POST.get('cache_key', '').strip()
    
    if not all([alert_id, start_date, end_date, cache_key]):
        return JsonResponse({
            'success': False,
            'message': 'Missing required parameters'
        })
    
    # ìºì‹œì—ì„œ DataFrame ê°€ì ¸ì˜¤ê¸°
    df = get_orderbook_dataframe(cache_key)
    
    if df is None:
        return JsonResponse({
            'success': False,
            'message': f'ìºì‹œëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cache_key}'
        })
    
    try:
        # ê¸°ê°„ í•„í„°ë§
        df_filtered = df[
            (pd.to_datetime(df['trade_date']) >= pd.to_datetime(start_date)) &
            (pd.to_datetime(df['trade_date']) <= pd.to_datetime(end_date))
        ].copy()
        
        if df_filtered.empty:
            return JsonResponse({
                'success': True,
                'alert_id': alert_id,
                'detail': {
                    'summary': {
                        'buy_amount': 0, 'buy_count': 0,
                        'sell_amount': 0, 'sell_count': 0,
                        'deposit_krw': 0, 'deposit_krw_count': 0,
                        'withdraw_krw': 0, 'withdraw_krw_count': 0,
                        'deposit_crypto': 0, 'deposit_crypto_count': 0,
                        'withdraw_crypto': 0, 'withdraw_crypto_count': 0
                    },
                    'by_ticker': {}
                }
            })
        
        # ë¶„ì„ ì‹¤í–‰
        analyzer = OrderbookAnalyzer(df_filtered)
        analyzer.analyze()
        patterns = analyzer.get_pattern_analysis()
        
        # ì¢…ëª©ë³„ ìƒì„¸ ì •ë¦¬
        by_ticker = {
            'buy': patterns.get('buy_details', [])[:10],
            'sell': patterns.get('sell_details', [])[:10],
            'deposit': patterns.get('deposit_crypto_details', [])[:10],
            'withdraw': patterns.get('withdraw_crypto_details', [])[:10]
        }
        
        # ì¢…ëª©ë³„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        for action in by_ticker:
            by_ticker[action] = [
                {
                    'ticker': ticker,
                    'amount': data['amount_krw'],
                    'count': data['count']
                }
                for ticker, data in by_ticker[action]
            ]
        
        detail = {
            'summary': {
                'buy_amount': patterns.get('total_buy_amount', 0),
                'buy_count': patterns.get('total_buy_count', 0),
                'sell_amount': patterns.get('total_sell_amount', 0),
                'sell_count': patterns.get('total_sell_count', 0),
                'deposit_krw': patterns.get('total_deposit_krw', 0),
                'deposit_krw_count': patterns.get('total_deposit_krw_count', 0),
                'withdraw_krw': patterns.get('total_withdraw_krw', 0),
                'withdraw_krw_count': patterns.get('total_withdraw_krw_count', 0),
                'deposit_crypto': patterns.get('total_deposit_crypto', 0),
                'deposit_crypto_count': patterns.get('total_deposit_crypto_count', 0),
                'withdraw_crypto': patterns.get('total_withdraw_crypto', 0),
                'withdraw_crypto_count': patterns.get('total_withdraw_crypto_count', 0)
            },
            'by_ticker': by_ticker
        }
        
        return JsonResponse({
            'success': True,
            'alert_id': alert_id,
            'period': f"{start_date} ~ {end_date}",
            'detail': detail
        })
        
    except Exception as e:
        logger.exception(f"Error analyzing alert orderbook: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}'
        })
    
# ========== íŒŒì¼ ëë¶€ë¶„ì— ì¶”ê°€ ==========

from .toml_exporter import toml_collector
import tempfile
from django.http import FileResponse

@login_required
@require_POST
def prepare_toml_data(request):
    """
    í™”ë©´ì— ë Œë”ë§ëœ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ TOML í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
    """
    try:
        # ì„¸ì…˜ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        session_data = {
            'alert_data': request.session.get('current_alert_data', {}),
            'customer_data': request.session.get('current_customer_data', {}),
            'corp_related_data': request.session.get('current_corp_related_data', {}),
            'person_related_data': request.session.get('current_person_related_data', {}),
            'rule_history_data': request.session.get('current_rule_history_data', {}),
            'orderbook_analysis': request.session.get('current_orderbook_analysis', {}),
            'stds_dtm_summary': request.session.get('current_stds_dtm_summary', {})
        }
        
        # TOML ë°ì´í„° ìˆ˜ì§‘
        collected_data = toml_collector.collect_all_data(session_data)
        # ë””ë²„ê¹…: ì²˜ë¦¬ëœ ë°ì´í„° ë¡œê¹…
        logger.info("=== TOML Data Processing Debug ===")
        logger.info(f"Customer data keys: {collected_data.get('data', {}).get('customer', {}).keys()}")
        logger.info(f"Orderbook summary: {collected_data.get('data', {}).get('orderbook', {}).get('summary_text', '')[:100]}")
        
               
        # ì„ì‹œ íŒŒì¼ì— ì €ì¥
        with tempfile.NamedTemporaryFile(mode='w', suffix='.toml', delete=False, encoding='utf-8') as tmp:
            import toml
            toml.dump(collected_data, tmp)
            tmp_path = tmp.name
        
        # ì„¸ì…˜ì— ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥
        request.session['toml_temp_path'] = tmp_path
        
        return JsonResponse({
            'success': True,
            'message': 'TOML ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ',
            'data_count': len(collected_data.get('data', {})),
            'selected_sections': toml_collector.data_selection
        })
        
    except Exception as e:
        logger.exception(f"Error preparing TOML data: {e}")
        return JsonResponse({
            'success': False,
            'message': f'TOML ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}'
        })


@login_required
def download_toml(request):
    """
    ì¤€ë¹„ëœ TOML íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    """
    try:
        tmp_path = request.session.get('toml_temp_path')
        if not tmp_path or not Path(tmp_path).exists():
            return JsonResponse({
                'success': False,
                'message': 'TOML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            })
        
        # Alert IDë¡œ íŒŒì¼ëª… ìƒì„±
        alert_id = request.session.get('current_alert_id', 'unknown')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'str_data_{alert_id}_{timestamp}.toml'
        
        # íŒŒì¼ ì‘ë‹µ ìƒì„±
        response = FileResponse(
            open(tmp_path, 'rb'),
            as_attachment=True,
            filename=filename
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ë‹¤ìš´ë¡œë“œ í›„)
        def cleanup():
            try:
                Path(tmp_path).unlink()
            except:
                pass
        
        import atexit
        atexit.register(cleanup)
        
        return response
        
    except Exception as e:
        logger.exception(f"Error downloading TOML: {e}")
        return JsonResponse({
            'success': False,
            'message': f'TOML ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
        })


@login_required
@require_POST
def analyze_stds_dtm_orderbook(request):
    """
    ëŒ€í‘œ ALERTì˜ STDS_DTM ë‚ ì§œì— ëŒ€í•œ Orderbook ìš”ì•½
    """
    stds_date = request.POST.get('stds_date', '').strip()
    cache_key = request.POST.get('cache_key', '').strip()
    
    if not all([stds_date, cache_key]):
        return JsonResponse({
            'success': False,
            'message': 'Missing required parameters'
        })
    
    # ìºì‹œì—ì„œ DataFrame ê°€ì ¸ì˜¤ê¸°
    df = get_orderbook_dataframe(cache_key)
    
    if df is None:
        return JsonResponse({
            'success': False,
            'message': f'ìºì‹œëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cache_key}'
        })
    
    try:
        # STDS_DTM ë‚ ì§œë¡œ í•„í„°ë§
        target_date = pd.to_datetime(stds_date).date()
        df_filtered = df[pd.to_datetime(df['trade_date']).dt.date == target_date].copy()
        
        if df_filtered.empty:
            return JsonResponse({
                'success': True,
                'date': stds_date,
                'summary': {
                    'total_records': 0,
                    'message': 'í•´ë‹¹ ë‚ ì§œì˜ ê±°ë˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
                }
            })
        
        # ë¶„ì„ ì‹¤í–‰
        analyzer = OrderbookAnalyzer(df_filtered)
        analyzer.analyze()
        patterns = analyzer.get_pattern_analysis()
        
        # ì¢…ëª©ë³„ ìƒì„¸ ì •ë¦¬
        summary = {
            'date': stds_date,
            'total_records': len(df_filtered),
            'buy_amount': patterns.get('total_buy_amount', 0),
            'buy_count': patterns.get('total_buy_count', 0),
            'buy_details': [
                {'ticker': t, 'amount': d['amount_krw'], 'count': d['count']}
                for t, d in (patterns.get('buy_details', [])[:10])
            ],
            'sell_amount': patterns.get('total_sell_amount', 0),
            'sell_count': patterns.get('total_sell_count', 0),
            'sell_details': [
                {'ticker': t, 'amount': d['amount_krw'], 'count': d['count']}
                for t, d in (patterns.get('sell_details', [])[:10])
            ],
            'deposit_krw_amount': patterns.get('total_deposit_krw', 0),
            'deposit_krw_count': patterns.get('total_deposit_krw_count', 0),
            'withdraw_krw_amount': patterns.get('total_withdraw_krw', 0),
            'withdraw_krw_count': patterns.get('total_withdraw_krw_count', 0),
            'deposit_crypto_amount': patterns.get('total_deposit_crypto', 0),
            'deposit_crypto_count': patterns.get('total_deposit_crypto_count', 0),
            'deposit_crypto_details': [
                {'ticker': t, 'amount': d['amount_krw'], 'count': d['count']}
                for t, d in (patterns.get('deposit_crypto_details', [])[:10])
            ],
            'withdraw_crypto_amount': patterns.get('total_withdraw_crypto', 0),
            'withdraw_crypto_count': patterns.get('total_withdraw_crypto_count', 0),
            'withdraw_crypto_details': [
                {'ticker': t, 'amount': d['amount_krw'], 'count': d['count']}
                for t, d in (patterns.get('withdraw_crypto_details', [])[:10])
            ]
        }
        
        # ì„¸ì…˜ì— ì €ì¥ (TOML ì €ì¥ìš©)
        request.session['current_stds_dtm_summary'] = summary
        
        return JsonResponse({
            'success': True,
            'summary': summary
        })
        
    except Exception as e:
        logger.exception(f"Error analyzing STDS_DTM orderbook: {e}")
        return JsonResponse({
            'success': False,
            'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}'
        })
    


@login_required
@require_POST
def save_to_session(request):
    """
    JavaScriptì—ì„œ ì„¸ì…˜ì— ë°ì´í„° ì €ì¥
    """
    key = request.POST.get('key', '').strip()
    data = request.POST.get('data', '').strip()
    
    if not key:
        return JsonResponse({
            'success': False,
            'message': 'Key is required'
        })
    
    try:
        # JSON íŒŒì‹±
        parsed_data = json.loads(data) if data else {}
        
        # ì„¸ì…˜ì— ì €ì¥
        request.session[key] = parsed_data
        
        # ì„¸ì…˜ ê°±ì‹ 
        request.session.modified = True
        
        logger.debug(f"Saved to session: {key} (size: {len(data)} bytes)")
        
        return JsonResponse({
            'success': True,
            'key': key
        })
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON decode error: {e}")
        return JsonResponse({
            'success': False,
            'message': f'Invalid JSON data: {e}'
        })
    except Exception as e:
        logger.exception(f"Error saving to session: {e}")
        return JsonResponse({
            'success': False,
            'message': f'Failed to save: {e}'
        })






